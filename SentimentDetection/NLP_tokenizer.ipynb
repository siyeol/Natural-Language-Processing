{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "process.py.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-Jl2XHQ_kPB"
      },
      "source": [
        "## requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0UHnZIX_kPC"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "!pip3 install JPype1-py3 konlpy pandas tqdm sklearn\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "!JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7zJx-70_kPF"
      },
      "source": [
        "## read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_4U4FOv_kPG"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/fllff/nlp_basic_lec/master/ratings_train.txt',sep='\\t')\n",
        "\n",
        "print(df.isnull().values.any())\n",
        "\n",
        "# NaN 삭제\n",
        "df = df.dropna()\n",
        "\n",
        "# shuffle\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT0qo7hr_kPJ"
      },
      "source": [
        "## tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a7rDIar_kPJ"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "\n",
        "mecab.morphs('한국어 토큰 분리기 사용방법')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "IcHnNF3j_kPM"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, row in tqdm(df.iterrows()):\n",
        "    df.at[i,'document'] = mecab.morphs(row['document'])\n",
        "    \n",
        "print(df)                                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "cW0YZ8Nl_kPP"
      },
      "source": [
        "doc_list = df['document']\n",
        "label_list = df['label']\n",
        "\n",
        "doc_len_list = [len(doc) for doc in doc_list]\n",
        "\n",
        "# 문장길이 분포 확인\n",
        "distribution = [0]*20\n",
        "for doc_len in doc_len_list:\n",
        "    distribution[int(doc_len/10)] += 1\n",
        "print(distribution)\n",
        "\n",
        "# label균형 확인\n",
        "label_list.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w36XMLCb_kPR"
      },
      "source": [
        "# 문장 길이 trim\n",
        "doc_list = [doc[:50] for doc in doc_list]\n",
        "\n",
        "# 문장길이 분포 확인\n",
        "distribution = [0]*20\n",
        "for doc in doc_list:\n",
        "    distribution[int(len(doc)/10)] += 1\n",
        "print(distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9_dyxzi_kPU"
      },
      "source": [
        "# Naive bayse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4a1w1Aw_kPU"
      },
      "source": [
        "## count vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Z2bkkqHT_kPX"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# vectorizer 선언\n",
        "cv = CountVectorizer(min_df=100)\n",
        "\n",
        "print(doc_list[0])\n",
        "print(' '.join(doc_list[0]))\n",
        "doc_list_joined = [' '.join(doc) for doc in doc_list]\n",
        "\n",
        "# counting 시작\n",
        "cv.fit(doc_list_joined)\n",
        "\n",
        "# 벡터화\n",
        "vec_list = cv.transform(doc_list_joined).toarray()\n",
        "\n",
        "print(len(cv.vocabulary_))\n",
        "print(vec_list[0][:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3WHZUkj_kPZ"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLCqDSeM_kPa"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(vec_list, label_list, test_size=0.1)\n",
        "mnb = MultinomialNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEvRigAE_kPc"
      },
      "source": [
        "y_pred = mnb.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "print(\"accuracy: {}\".format((y_test == y_pred).sum() / len(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS1Zj_NU_kPe"
      },
      "source": [
        "# Deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fDBzKnY_kPf"
      },
      "source": [
        "## w2v vectorization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR8Pe4KZ_kPg"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "word_model = Word2Vec(doc_list, size=16, window=5, min_count=50, workers=2, iter=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlD-dxFd_kPi"
      },
      "source": [
        "word_model.wv.most_similar('배우')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x74Ot8yQ_kPk"
      },
      "source": [
        "word_model.wv['배우']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PaR2ayhG_kPm"
      },
      "source": [
        "# token -> vector 변환\n",
        "vector_list = []\n",
        "for doc in tqdm(doc_list):\n",
        "    tmp = []\n",
        "    for token in doc:\n",
        "        try:\n",
        "            tmp.append(word_model.wv[token].tolist())\n",
        "        except:\n",
        "            tmp.append([0]*16)\n",
        "        \n",
        "    vector_list.append(tmp)\n",
        "    \n",
        "print(vector_list[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELZ0juOW_kPp"
      },
      "source": [
        "MAX_LEN = 50\n",
        "tmp=[]\n",
        "\n",
        "for i, vector in tqdm(enumerate(vector_list)):\n",
        "    vector_list[i].extend([[0]*16 for x in range(MAX_LEN-len(vector))])\n",
        "\n",
        "print(len(vector_list[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6PMbSEx_kPr"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxaNNC8G_kPs"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrCgR6S6_kPu"
      },
      "source": [
        "# create model\n",
        "inputs = keras.Input(shape=(50,16))\n",
        "#x = layers.SimpleRNN(16)(inputs)\n",
        "x = layers.LSTM(16)(inputs)\n",
        "x = layers.Dense(1)(x)\n",
        "outputs = keras.activations.sigmoid(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()\n",
        "keras.utils.plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvh14Fmu_kPx"
      },
      "source": [
        "model.compile(optimizer='Adam',\n",
        "              metrics=['accuracy'],\n",
        "              loss=keras.losses.BinaryCrossentropy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk0xN0vi_kP0"
      },
      "source": [
        "import datetime\n",
        "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "model.fit(np.asarray(vector_list), \n",
        "          np.asarray(label_list), \n",
        "          batch_size=32, \n",
        "          epochs=20, \n",
        "          validation_split=0.1,\n",
        "          callbacks=tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNqAuAS3_kP1"
      },
      "source": [
        "## predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umjSRe55_kP2"
      },
      "source": [
        "sentence = '난 좀 지루한 느낌'\n",
        "\n",
        "test_vector = []\n",
        "for token in mecab.morphs(sentence):\n",
        "    try:\n",
        "        test_vector.append(word_model.wv[token].tolist())\n",
        "    except:\n",
        "        test_vector.append([0]*16)\n",
        "\n",
        "test_vector.extend([[0]*16 for x in range(MAX_LEN-len(test_vector))])\n",
        "print(test_vector)\n",
        "model.predict(np.asarray([test_vector]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOKZ9a1U_kP4"
      },
      "source": [
        "## tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmMzJsPy_kP4"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}